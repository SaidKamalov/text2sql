{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_llm import infer_llm\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from enums import InferenceOptions\n",
    "from utils import post_process_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "\n",
    "PROMPTS_FILE = './pipeline_results/prompts.json'\n",
    "\n",
    "INFERENCE_MODELS = [InferenceOptions.SQL_UNI, InferenceOptions.SQL_REASON]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: openai$full_schema\n",
      "Key: reasoning$full_schema\n",
      "Key: openai$full_schema$fk\n",
      "Key: reasoning$full_schema$fk\n",
      "Key: openai$full_schema$fk$random_hardness$3\n",
      "Key: openai$full_schema$fk$random_hardness$5\n",
      "Key: openai$full_schema$fk$cosine_sim_hardness$3\n",
      "Key: openai$full_schema$fk$cosine_sim_hardness$5\n",
      "Key: reasoning$full_schema$fk$random_hardness$3\n",
      "Key: reasoning$full_schema$fk$random_hardness$5\n",
      "Key: reasoning$full_schema$fk$cosine_sim_hardness$3\n",
      "Key: reasoning$full_schema$fk$cosine_sim_hardness$5\n"
     ]
    }
   ],
   "source": [
    "with open(PROMPTS_FILE, 'r') as f:\n",
    "    list_of_dicts = json.load(f)\n",
    "\n",
    "prompt_merged_dict = {}\n",
    "for d in list_of_dicts:\n",
    "    prompt_merged_dict.update(d)\n",
    "\n",
    "for k in prompt_merged_dict.keys():\n",
    "    print(f'Key: {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model to prompt mapping: {'gpt-4.1-2025-04-14': ['openai$full_schema', 'openai$full_schema$fk', 'openai$full_schema$fk$random_hardness$3', 'openai$full_schema$fk$random_hardness$5', 'openai$full_schema$fk$cosine_sim_hardness$3', 'openai$full_schema$fk$cosine_sim_hardness$5'], 'o4-mini-2025-04-16': ['reasoning$full_schema', 'reasoning$full_schema$fk', 'reasoning$full_schema$fk$random_hardness$3', 'reasoning$full_schema$fk$random_hardness$5', 'reasoning$full_schema$fk$cosine_sim_hardness$3', 'reasoning$full_schema$fk$cosine_sim_hardness$5']}\n"
     ]
    }
   ],
   "source": [
    "model_to_promp = {\n",
    "    InferenceOptions.SQL_UNI.value: [k for k in prompt_merged_dict.keys() if k.startswith('openai')], \n",
    "    InferenceOptions.SQL_REASON.value: [k for k in prompt_merged_dict.keys() if k.startswith('reasoning')]\n",
    "}\n",
    "print(f'Model to prompt mapping: {model_to_promp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV_MESSAGE: You are an expert SQL translator specialized in converting natural language queries into correct and efficient SQL statements.\n",
      "Pay attention to PROVIDED INFORMATION and EXAMPLES.\n",
      "If aliases for table names are needed use: T1, T2 ... as aliases!\n",
      "\n",
      "# Response Formats\n",
      "\n",
      "## response_format_schema\n",
      "{\"type\": \"object\",\"properties\": {\"sql_query\": {\"type\": \"string\",},}}\n"
     ]
    }
   ],
   "source": [
    "DEV_MESSAGE = \"You are an expert SQL translator specialized in converting natural language queries into correct and efficient SQL statements.\\n\"\n",
    "DEV_MESSAGE += \"Pay attention to PROVIDED INFORMATION and EXAMPLES.\\n\"\n",
    "DEV_MESSAGE += \"If aliases for table names are needed use: T1, T2 ... as aliases!\\n\\n\"\n",
    "DEV_MESSAGE += \"\"\"# Response Formats\\n\\n## response_format_schema\\n{\"type\": \"object\",\"properties\": {\"sql_query\": {\"type\": \"string\",},}}\"\"\"\n",
    "print(f'DEV_MESSAGE: {DEV_MESSAGE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring SQL queries: 100%|██████████| 1034/1034 [24:32<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "oai_prompts = prompt_merged_dict['openai$full_schema']\n",
    "oai_base_responces = infer_llm(prompts=oai_prompts, dev_message=DEV_MESSAGE, inference_option=InferenceOptions.SQL_UNI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for p in list(oai_base_responces):\n",
    "    try:\n",
    "        responce_string = p.output[0].content[0].text\n",
    "    except:\n",
    "        responce_string = \"{'sql_query': ''}\"\n",
    "    response_dict = json.loads(responce_string)\n",
    "    pred_sql = 'SELECT '+ response_dict['sql_query']\n",
    "    pred.append(post_process_sql(pred_sql))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = './pipeline_results/dev_pred_oai_base_v0.txt'\n",
    "with open(output_file, \"w\") as f:\n",
    "    for line in pred:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826\n",
      "827\n"
     ]
    }
   ],
   "source": [
    "\"SELECT name FROM conductor WHERE nationality != 'USA'\"\n",
    "for i, p in enumerate(pred):\n",
    "    if p == \"SELECT name FROM conductor WHERE nationality != 'USA'\":\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring SQL queries: 100%|██████████| 1034/1034 [1:20:58<00:00,  4.70s/it]\n"
     ]
    }
   ],
   "source": [
    "reasoning_prompts = prompt_merged_dict['reasoning$full_schema']\n",
    "reasoning_base_responces = infer_llm(prompts=reasoning_prompts, dev_message=DEV_MESSAGE, inference_option=InferenceOptions.SQL_REASON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseReasoningItem(id='rs_6814f0cdae988191a562f9bf2b3e1c7d0ad775f874f68bc9', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f0d564048191abd872781a536f180eb243310bc2e59f', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f0dcc5f481918a74e8d08d33a3a2071b20b1d043bd36', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f0fa27a48191a38b24a1d020448f0adaafef88fef6c3', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f104a4388191b793c16829e851f10e0525ae333542ba', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f1480bc88191b4583e7ad3f5f40c0b391d2a85fe8b89', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f14fa9408191a562b748c1e1dc090b3b297dc7dc0a7b', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f1c0bbf48191b2dc4e71e6267ebe05a861ffe4324360', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f1ca283c8191bc1df4fcf975b0e10929c672dc22a6fd', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f1e74e248191964284f05ff5ccf8079983018fcbdde5', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f1f12d548191a72b227187c440b8024a205c38e95f68', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f2369ea88191ba5844a0905bfe8e06c18c2fd35f5378', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f23f54d88191b7140015caa3d02c03de08188f5a1e41', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f256cfb881918f51e2d68fb0452a024a8869b09cd22f', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f26104fc81918ec92db4ee6f83590fbda126b3eb215d', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f289b510819190d271d1f3491cf70babf1ef1bb7077c', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f291552c8191bf2f6d2f192292d10a47e0be3b7fd173', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f2d574608191b4fef16e52ed6dec071fb3b9f2245ff4', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f2df1ba48191b2f543376f4ada5b0f20bb773f7012e1', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f2f58420819186c303fa2b73bb5c00bf6326fa386767', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f3dbfe688191a1b25f78a23538d60a74c394c2a70f6b', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f3e284e08191ad022ac5f5b85b1a0475bdcea376ea37', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f4e04de48191a0435157550be3bd04de1c1c485b05e1', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f52809c48191964cb5af84a9998a0ca06ed5ab6f2a48', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f5e4bdbc819196a378f6a8b8cb890a6f2045d8204e92', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f74d99b081918d05e4561c6f58bc0d4c83150480ee94', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f7f4c5d081919cb197ed090cb99b02acc9562d92221b', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f7fddc7c8191afdd17eed781615104a6156a1ac68305', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f903f5e88191be73569904ae021602400b69f2de5560', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f9553b008191bcb200158e928f6903ac302fb0513092', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814f992116881918e8300e92fdee538038a9c140d0d5c48', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814fa1fa5a08191bb4f13b003b20e9f04f1d8672e1d1d09', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814fa26f8848191bddf1d4fcdb083490c8d2ea186d871ac', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814fa31931081918c9c9c1ac0309f2503180c9ad2ce785a', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814fc27dfa08191b58745ed80ae3d12000bf8bc3dcb88c4', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6814fc37182c819199a0a37f337e63320daafa1ea24a889a', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6815003ed2e881919253b066a7eb5ad40eb28b231e5454c9', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_681500d699b4819181cd6a77e7685f110d5ba339c5ea7948', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_681500de67a881918bf02905ccaaec6d0ed6e074c27dcaa4', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_68150147f0f88191bd73f28e4a5638da058dba836e40186f', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_6815014f03f08191821275c93fc2d24000cdde759583f5f1', summary=[], type='reasoning', status=None)]\n",
      "[ResponseReasoningItem(id='rs_681502144f5c8191ab5ffa684b7a32350c65270d8e75cc67', summary=[], type='reasoning', status=None)]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for p in list(reasoning_base_responces):\n",
    "    try:\n",
    "        responce_string = p.output[1].content[0].text\n",
    "    except:\n",
    "        print(p.output)\n",
    "        responce_string = '{\"sql_query\": \"\"}'\n",
    "    try:\n",
    "        response_dict = json.loads(post_process_sql(responce_string))\n",
    "    except:\n",
    "        response_dict = json.loads(post_process_sql(responce_string) + '\"}')\n",
    "    pred_sql = 'SELECT '+ response_dict['sql_query']\n",
    "    pred.append(post_process_sql(pred_sql))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = './pipeline_results/dev_pred_reason_base_v0.txt'\n",
    "with open(output_file, \"w\") as f:\n",
    "    for line in pred:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASE + META INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring SQL queries: 100%|██████████| 1034/1034 [27:54<00:00,  1.62s/it] \n"
     ]
    }
   ],
   "source": [
    "oai_base_meta_prompts = prompt_merged_dict['openai$full_schema$fk']\n",
    "oai_base_meta_responces = infer_llm(prompts=oai_base_meta_prompts, dev_message=DEV_MESSAGE, inference_option=InferenceOptions.SQL_UNI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for p in list(oai_base_meta_responces):\n",
    "    try:\n",
    "        responce_string = p.output[0].content[0].text\n",
    "    except:\n",
    "        responce_string = \"{'sql_query': ''}\"\n",
    "    response_dict = json.loads(responce_string)\n",
    "    pred_sql = 'SELECT '+ response_dict['sql_query']\n",
    "    pred.append(post_process_sql(pred_sql))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = './pipeline_results/dev_pred_oai_base_meta_v0.txt'\n",
    "with open(output_file, \"w\") as f:\n",
    "    for line in pred:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE + META INFO + EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OAI + random by hardness k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring SQL queries: 100%|██████████| 1034/1034 [24:07<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "oai_base_meta_rand_hard_3_prompts = prompt_merged_dict['openai$full_schema$fk$random_hardness$3']\n",
    "oai_base_meta_rand_hard_3_responces = infer_llm(prompts=oai_base_meta_rand_hard_3_prompts, dev_message=DEV_MESSAGE, inference_option=InferenceOptions.SQL_UNI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for p in list(oai_base_meta_rand_hard_3_responces):\n",
    "    try:\n",
    "        responce_string = p.output[0].content[0].text\n",
    "    except:\n",
    "        responce_string = \"{'sql_query': ''}\"\n",
    "    response_dict = json.loads(responce_string)\n",
    "    pred_sql = 'SELECT '+ response_dict['sql_query']\n",
    "    pred.append(post_process_sql(pred_sql))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = './pipeline_results/dev_pred_oai_base_meta_rand_hard_3_v0.txt'\n",
    "with open(output_file, \"w\") as f:\n",
    "    for line in pred:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OAI + random by hardness k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring SQL queries: 100%|██████████| 1034/1034 [24:07<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "oai_base_meta_rand_hard_5_prompts = prompt_merged_dict['openai$full_schema$fk$random_hardness$5']\n",
    "oai_base_meta_rand_hard_5_responces = infer_llm(prompts=oai_base_meta_rand_hard_5_prompts, dev_message=DEV_MESSAGE, inference_option=InferenceOptions.SQL_UNI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for p in list(oai_base_meta_rand_hard_5_responces):\n",
    "    try:\n",
    "        responce_string = p.output[0].content[0].text\n",
    "    except:\n",
    "        responce_string = \"{'sql_query': ''}\"\n",
    "    response_dict = json.loads(responce_string)\n",
    "    pred_sql = 'SELECT '+ response_dict['sql_query']\n",
    "    pred.append(post_process_sql(pred_sql))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = './pipeline_results/dev_pred_oai_base_meta_rand_hard_5_v0.txt'\n",
    "with open(output_file, \"w\") as f:\n",
    "    for line in pred:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
